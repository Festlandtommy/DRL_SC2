\relax 
\babel@aux{english}{}
\citation{Rojas1996}
\citation{Patterson1997}
\citation{Patterson1997}
\citation{Rojas1996}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Artifical Neural Networks}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Biological Paradigm}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schemantic depiction of a neuron}}{1}}
\newlabel{fig:neuron_schemantic}{{1}{1}}
\citation{Patterson1997}
\citation{Bartz2018}
\citation{Schwenker2001}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Artifical Neurons}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Three stage model of an artifical neuron \cite  {Bartz2018}.}}{2}}
\newlabel{fig:artifical_neuron}{{2}{2}}
\newlabel{eq:net}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multi Layer Perceptron}{2}}
\citation{Rumelhart1985}
\citation{Patterson1997}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Activation Functions}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Optimizing Neural Networks}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Deep Learning}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Supervised Learning}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unsupervised Learning}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep Reinforcement Learning}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schemantic depiction of a reinforcement learning process.}}{4}}
\newlabel{fig:drl_schemantic}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Markov Decision Process}{4}}
\newlabel{eq:markov_prop}{{2}{4}}
\newlabel{eq:trans_prob}{{3}{4}}
\newlabel{eq:trans_matrix}{{4}{4}}
\newlabel{eq:reward_func}{{5}{4}}
\citation{Sutton2015}
\newlabel{eq:markov_reward}{{6}{5}}
\newlabel{eq:disc_markov_reward}{{7}{5}}
\newlabel{eq:value_func}{{8}{5}}
\newlabel{eq:bellman_matrix}{{9}{5}}
\newlabel{eq:trans_prob_with_action}{{10}{5}}
\newlabel{eq:reward_func_with_action}{{11}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Q-Learning}{5}}
\newlabel{eq:q_learn}{{12}{5}}
\citation{VanHasselt2015}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Q-learning for estimating a policy $\pi $}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Double-Q-Learning}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}StarCraft II}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Double Q-learning}}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Involved Systems}{7}}
\citation{Mnih2016}
\citation{Bartz2018}
\bibstyle{abbrvdin}
\bibdata{researchProject}
\bibcite{Bartz2018}{1}
\bibcite{VanHasselt2015}{2}
\bibcite{Mnih2016}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Ray Server}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}RLlib}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}StarCraft II Machine Learning API}{8}}
\newlabel{sec:SC2API}{{6.3}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}StarCraft II Learning Environment}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Optimization Szenario}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Experiment Results}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Learnings}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion}{8}}
\bibcite{Patterson1997}{4}
\bibcite{Rojas1996}{5}
\bibcite{Schwenker2001}{6}
\bibcite{Sutton2015}{7}
